{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2de18c-f8e2-406b-9ecd-eef0e9777fed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install deepl decouple contractions spacy tqdm deep-translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eaa63ca-b26f-4eab-843f-b1812ece85f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import contractions\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import text2emotion as te\n",
    "\n",
    "from deep_translator import GoogleTranslator as Translator\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8164ad97-46a3-4670-902a-7844c9951fa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d87b4-4a02-4fea-a23c-3cb7320abfb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get datasets into single dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7554db43-9d9d-489c-ab77-156bc9f110fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path=\"./data-extraction/\"\n",
    "df_dict = {}\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file_name in files:\n",
    "        if file_name.endswith('.jl') or file_name.endswith('.jsonl'):\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            name = os.path.splitext(file_name)[0]\n",
    "            df_dict[name] = pd.read_json(path_or_buf=file_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bcd09f83-a701-42eb-8f68-3cc931e973b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4559e2b2-7d82-4db4-b17d-d8644a017da5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for df in df_dict:\n",
    "    df_dict[df] = df_dict[df].drop(\"url\", axis=1).drop(\"_type\", axis=1).drop(\"cashtags\", axis=1) #.head() # to sample the data down for testing this monster\n",
    "    keys = df_dict[df].keys()\n",
    "    #print(f\"{df} {keys}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deca436b-3961-439d-a527-c29cfc6fa0de",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2dfe3050-9dcf-46c3-9e44-d8cfc36e7c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_strings(column: str, to_replace: str, value: str, regex: bool) -> pd.DataFrame:\n",
    "    return df[column].replace(to_replace=to_replace, value=value, regex=regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09bdd537-71bd-4b66-b6bd-d2024c67cc3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'\\n', ' ', text)\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16faded4-f133-4fc9-a0a2-6462f7996726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_pascal_case(word):\n",
    "    return re.sub(r'(?<=\\w)([A-Z])', r' \\1', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b2313c6-6cbc-4a6d-9aae-765255394a3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "translator = Translator(target=\"en\")\n",
    "def get_translations(x):\n",
    "    if x[\"lang\"] == \"en\":\n",
    "        return x[\"cleanedContent\"]\n",
    "    else:\n",
    "        translation = translator.translate(x[\"cleanedContent\"])\n",
    "        return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b91325f0-921c-4682-a494-7e9168fc9d10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current dataframe: twitter-pfizer\n",
      "Current dataframe: tw_hshtag_covid19\n",
      "Current dataframe: tw_hshtag_monkeypox\n",
      "Current dataframe: tw_hshtag_flu\n"
     ]
    }
   ],
   "source": [
    "for frame in df_dict:\n",
    "    print(f\"Current dataframe: {frame}\")\n",
    "\n",
    "    df_dict[frame].fillna(value='', inplace=True)\n",
    "    \n",
    "    # Convert column to string type\n",
    "    df_dict[frame]['cleanedContent'] = df_dict[frame]['rawContent'].astype(str)\n",
    "    \n",
    "    # Clean and translate the data\n",
    "    df_dict[frame][\"cleanedContent\"] = df_dict[frame]['cleanedContent'].str.replace(r'\\bmpox\\b', 'monkeypox', regex=True)\n",
    "    df_dict[frame][\"cleanedContent\"] = df_dict[frame]['cleanedContent'].apply(split_pascal_case)\n",
    "    df_dict[frame][\"cleanedContent\"] = df_dict[frame]['cleanedContent'].apply(remove_special_characters)\n",
    "    # df_dict[frame][\"translatedContent\"] = df_dict[frame].apply(get_translations, axis=1)\n",
    "    \n",
    "    # Add tokenized column\n",
    "    df_dict[frame][\"tokens\"] = df_dict[frame][\"cleanedContent\"].apply(word_tokenize)\n",
    "    \n",
    "    # Save to files\n",
    "    df_dict[frame].to_json(f\"./data-cleaned/processed_{frame}.json\", orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81156ed1-056d-4e3c-9def-1d79174649f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the pfizer dataset without translation and just do a emotion check, what pfizer sentiment is"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
